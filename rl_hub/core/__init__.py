from pydantic import BaseModel, ConfigDict
import numpy as np


class Trajectory(BaseModel):
    """A single agent trajectory generated by the environment."""

    idx: int
    action: int
    state: np.ndarray
    reward: float

    model_config = ConfigDict(arbitrary_types_allowed=True)


class History(BaseModel):
    """A set of agent trajectories."""

    items: list[Trajectory]

    def actions(self) -> list[int]:
        """Returns the actions for each trajectory."""
        return [t.action for t in self.items]

    def states(self) -> list[np.ndarray]:
        """Returns the states for each trajectory."""
        return [t.state for t in self.items]

    def rewards(self) -> list[int]:
        """Returns the rewards for each trajectory."""
        return [t.reward for t in self.items]

    def returns(self, gamma: float = 1) -> list[float]:
        """Computes the discounted return for each trajectory."""
        discounted_returns = [0.0] * len(self.items)
        G = 0.0

        for T in reversed(self.items):
            discounted_returns[T.idx] = T.reward + gamma * G

        return discounted_returns
