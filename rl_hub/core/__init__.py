from pydantic import BaseModel, ConfigDict
import numpy as np


class Trajectory(BaseModel):
    """A single agent trajectory generated by the environment."""

    action: int
    state: np.ndarray
    reward: float

    model_config = ConfigDict(arbitrary_types_allowed=True)


class History(BaseModel):
    """A set of agent trajectories."""

    items: list[Trajectory]

    def actions(self) -> list[int]:
        """Returns the actions for each trajectory."""
        return [t.action for t in self.items]

    def states(self) -> list[np.ndarray]:
        """Returns the states for each trajectory."""
        return [t.state for t in self.items]

    def rewards(self) -> list[int]:
        """Returns the rewards for each trajectory."""
        return [t.reward for t in self.items]

    def returns(self, gamma: float = 1) -> list[float]:
        """
        Computes the discounted return for each trajectory. Starts at last trajectory and iterates backwards through time $t$.

        For every trajectory:
        $$
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
        $$

        When iterating, this simplifies to use future return:
        $$
        G_t = R_{t} + \gamma G_{t+1}
        $$
        """
        n = len(self.items)
        G = [0.0] * n

        G[n - 1] = self.items[n - 1].reward

        for t in range(n - 2, -1, -1):
            G[t] = self.items[t].reward + gamma * G[t + 1]

        return G
